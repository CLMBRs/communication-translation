nohup: ignoring input
Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['gumbel_encoder.layers.3.self_attn_layer_norm.bias', 'gumbel_encoder.layers.4.self_attn.k_proj.weight', 'gumbel_encoder.layers.9.final_layer_norm.weight', 'gumbel_encoder.layers.6.fc2.bias', 'gumbel_encoder.layers.0.fc1.weight', 'gumbel_encoder.layers.4.self_attn.k_proj.bias', 'gumbel_encoder.layers.7.self_attn_layer_norm.weight', 'gumbel_encoder.layers.0.self_attn.k_proj.bias', 'gumbel_encoder.layers.9.self_attn.q_proj.bias', 'gumbel_encoder.layers.1.fc1.bias', 'gumbel_encoder.layers.4.self_attn.out_proj.bias', 'gumbel_encoder.layers.8.self_attn_layer_norm.bias', 'gumbel_encoder.layers.6.fc1.bias', 'gumbel_encoder.layers.8.self_attn.v_proj.weight', 'gumbel_encoder.layers.5.final_layer_norm.weight', 'gumbel_encoder.layers.4.fc2.weight', 'gumbel_encoder.layers.7.fc1.weight', 'gumbel_encoder.layers.3.self_attn.q_proj.weight', 'gumbel_encoder.layers.10.self_attn.v_proj.bias', 'gumbel_encoder.layers.6.fc2.weight', 'gumbel_encoder.layers.2.fc1.bias', 'gumbel_encoder.layers.2.final_layer_norm.weight', 'gumbel_encoder.layers.10.self_attn.out_proj.weight', 'gumbel_encoder.layers.1.self_attn.q_proj.bias', 'gumbel_encoder.layers.3.self_attn.out_proj.weight', 'gumbel_encoder.layers.3.fc1.bias', 'gumbel_encoder.layers.1.fc2.weight', 'gumbel_encoder.layers.11.final_layer_norm.bias', 'gumbel_encoder.layers.2.self_attn.k_proj.bias', 'gumbel_encoder.layers.0.self_attn.out_proj.bias', 'gumbel_encoder.layers.2.final_layer_norm.bias', 'gumbel_encoder.layers.8.self_attn.k_proj.weight', 'gumbel_encoder.layers.2.self_attn.v_proj.weight', 'gumbel_encoder.layers.7.self_attn.out_proj.weight', 'gumbel_encoder.layers.11.self_attn.out_proj.weight', 'gumbel_encoder.layers.4.self_attn.q_proj.weight', 'gumbel_encoder.layers.10.self_attn_layer_norm.weight', 'gumbel_encoder.layers.11.fc2.weight', 'gumbel_encoder.layers.10.final_layer_norm.bias', 'gumbel_encoder.layers.4.fc1.weight', 'gumbel_encoder.layers.4.self_attn_layer_norm.weight', 'gumbel_encoder.layers.9.self_attn.k_proj.weight', 'gumbel_encoder.layers.5.self_attn_layer_norm.weight', 'gumbel_encoder.layers.1.self_attn.v_proj.weight', 'gumbel_encoder.layers.0.self_attn.k_proj.weight', 'gumbel_encoder.layers.3.self_attn.out_proj.bias', 'gumbel_encoder.layers.7.self_attn.v_proj.weight', 'gumbel_encoder.layers.6.self_attn.q_proj.weight', 'gumbel_encoder.layers.2.self_attn.k_proj.weight', 'gumbel_encoder.layers.7.fc2.bias', 'gumbel_encoder.layers.9.self_attn.out_proj.weight', 'gumbel_encoder.layers.10.fc2.bias', 'gumbel_encoder.layers.8.final_layer_norm.weight', 'gumbel_encoder.layers.1.self_attn.k_proj.bias', 'gumbel_encoder.layers.10.self_attn_layer_norm.bias', 'gumbel_encoder.layers.10.final_layer_norm.weight', 'gumbel_encoder.layers.5.self_attn.v_proj.weight', 'gumbel_encoder.layers.3.fc2.bias', 'gumbel_encoder.layers.6.final_layer_norm.bias', 'gumbel_encoder.layers.5.fc1.weight', 'gumbel_encoder.layers.5.self_attn_layer_norm.bias', 'gumbel_encoder.layers.11.self_attn_layer_norm.weight', 'embed_tokens.weight', 'gumbel_encoder.layers.5.self_attn.k_proj.bias', 'gumbel_encoder.layers.5.fc2.weight', 'gumbel_encoder.layers.3.fc2.weight', 'gumbel_encoder.layers.11.self_attn.q_proj.bias', 'gumbel_encoder.layers.3.self_attn.k_proj.bias', 'gumbel_encoder.layers.2.self_attn_layer_norm.bias', 'gumbel_encoder.layers.10.fc1.weight', 'gumbel_encoder.layers.11.self_attn.q_proj.weight', 'gumbel_encoder.layers.4.self_attn_layer_norm.bias', 'gumbel_encoder.layers.6.self_attn.q_proj.bias', 'gumbel_encoder.layers.8.self_attn.q_proj.bias', 'gumbel_encoder.layers.8.fc2.weight', 'gumbel_encoder.layers.4.self_attn.v_proj.weight', 'gumbel_encoder.layers.1.self_attn_layer_norm.bias', 'gumbel_encoder.layers.7.final_layer_norm.weight', 'gumbel_encoder.layers.6.self_attn.k_proj.bias', 'gumbel_encoder.layers.9.self_attn.q_proj.weight', 'gumbel_encoder.layers.4.final_layer_norm.bias', 'gumbel_encoder.layers.10.fc2.weight', 'gumbel_encoder.layers.4.self_attn.q_proj.bias', 'gumbel_encoder.layers.4.fc2.bias', 'gumbel_encoder.layers.1.fc1.weight', 'gumbel_encoder.layers.3.self_attn.k_proj.weight', 'gumbel_encoder.layers.1.final_layer_norm.bias', 'gumbel_encoder.layers.8.self_attn_layer_norm.weight', 'gumbel_encoder.layers.10.fc1.bias', 'gumbel_encoder.layers.0.self_attn_layer_norm.weight', 'gumbel_encoder.layers.0.self_attn.q_proj.bias', 'gumbel_encoder.embed_tokens.weight', 'gumbel_encoder.layers.8.fc1.weight', 'gumbel_encoder.layernorm_embedding.weight', 'gumbel_encoder.layers.6.fc1.weight', 'gumbel_encoder.layers.2.fc2.bias', 'gumbel_encoder.layers.11.self_attn.k_proj.weight', 'gumbel_encoder.layers.1.self_attn.out_proj.bias', 'gumbel_encoder.layers.9.self_attn.v_proj.bias', 'gumbel_encoder.layers.10.self_attn.k_proj.bias', 'gumbel_encoder.layers.11.self_attn_layer_norm.bias', 'gumbel_encoder.layers.9.self_attn.out_proj.bias', 'gumbel_encoder.layers.5.fc2.bias', 'gumbel_encoder.layers.3.self_attn.q_proj.bias', 'gumbel_encoder.layers.6.self_attn.v_proj.bias', 'gumbel_encoder.layers.1.self_attn.v_proj.bias', 'gumbel_encoder.layers.11.final_layer_norm.weight', 'gumbel_encoder.layers.7.self_attn.k_proj.bias', 'gumbel_encoder.layers.5.self_attn.q_proj.bias', 'gumbel_encoder.layers.1.self_attn.k_proj.weight', 'gumbel_encoder.layers.6.self_attn.out_proj.bias', 'gumbel_encoder.layers.3.final_layer_norm.bias', 'gumbel_encoder.layers.5.self_attn.out_proj.weight', 'gumbel_encoder.layers.6.self_attn.k_proj.weight', 'gumbel_encoder.layers.8.self_attn.q_proj.weight', 'gumbel_encoder.layers.2.self_attn.out_proj.weight', 'gumbel_encoder.layers.0.fc1.bias', 'gumbel_encoder.layers.7.fc2.weight', 'gumbel_encoder.layers.9.fc1.weight', 'gumbel_encoder.layers.7.self_attn.q_proj.weight', 'gumbel_encoder.layers.4.fc1.bias', 'gumbel_encoder.layers.11.self_attn.v_proj.bias', 'gumbel_encoder.layers.9.fc2.bias', 'gumbel_encoder.layers.8.fc2.bias', 'gumbel_encoder.layers.9.self_attn_layer_norm.weight', 'gumbel_encoder.layers.11.fc2.bias', 'gumbel_encoder.layers.11.fc1.bias', 'gumbel_encoder.layers.1.self_attn.out_proj.weight', 'gumbel_encoder.layers.3.final_layer_norm.weight', 'gumbel_encoder.layers.0.self_attn.v_proj.bias', 'gumbel_encoder.layers.3.fc1.weight', 'gumbel_encoder.layers.11.self_attn.k_proj.bias', 'gumbel_encoder.layers.8.final_layer_norm.bias', 'gumbel_encoder.layers.5.self_attn.out_proj.bias', 'gumbel_encoder.layers.7.fc1.bias', 'gumbel_encoder.layers.8.self_attn.out_proj.weight', 'gumbel_encoder.layers.0.self_attn.q_proj.weight', 'gumbel_encoder.layers.5.self_attn.v_proj.bias', 'gumbel_encoder.layers.9.self_attn.k_proj.bias', 'gumbel_encoder.layers.4.self_attn.v_proj.bias', 'gumbel_encoder.layers.2.fc1.weight', 'gumbel_encoder.layers.6.self_attn_layer_norm.weight', 'gumbel_encoder.layers.8.self_attn.out_proj.bias', 'gumbel_encoder.layers.7.self_attn.v_proj.bias', 'gumbel_encoder.layers.0.fc2.weight', 'gumbel_encoder.layers.6.final_layer_norm.weight', 'gumbel_encoder.layers.1.self_attn.q_proj.weight', 'gumbel_encoder.layers.9.final_layer_norm.bias', 'gumbel_encoder.layers.11.self_attn.out_proj.bias', 'gumbel_encoder.layers.5.fc1.bias', 'gumbel_encoder.layers.0.final_layer_norm.weight', 'gumbel_encoder.layers.9.self_attn.v_proj.weight', 'gumbel_encoder.layers.9.fc2.weight', 'gumbel_encoder.layers.2.self_attn.q_proj.bias', 'gumbel_encoder.layers.5.self_attn.k_proj.weight', 'gumbel_encoder.layers.4.final_layer_norm.weight', 'gumbel_encoder.layers.5.self_attn.q_proj.weight', 'gumbel_encoder.layers.1.fc2.bias', 'gumbel_encoder.layernorm_embedding.bias', 'gumbel_encoder.layers.3.self_attn.v_proj.bias', 'gumbel_encoder.embed_positions.weight', 'gumbel_encoder.layers.2.self_attn.v_proj.bias', 'gumbel_encoder.layers.2.self_attn.out_proj.bias', 'gumbel_encoder.layers.9.self_attn_layer_norm.bias', 'gumbel_encoder.layers.10.self_attn.q_proj.weight', 'gumbel_encoder.layers.11.fc1.weight', 'gumbel_encoder.layers.7.final_layer_norm.bias', 'gumbel_encoder.layers.10.self_attn.k_proj.weight', 'gumbel_encoder.layers.7.self_attn_layer_norm.bias', 'gumbel_encoder.layers.2.self_attn.q_proj.weight', 'gumbel_encoder.layers.6.self_attn.out_proj.weight', 'gumbel_encoder.layers.7.self_attn.k_proj.weight', 'gumbel_encoder.layers.9.fc1.bias', 'gumbel_encoder.layers.0.self_attn.v_proj.weight', 'gumbel_encoder.layers.3.self_attn.v_proj.weight', 'gumbel_encoder.layers.0.self_attn_layer_norm.bias', 'gumbel_encoder.layers.4.self_attn.out_proj.weight', 'gumbel_encoder.layers.10.self_attn.q_proj.bias', 'gumbel_encoder.layers.10.self_attn.out_proj.bias', 'gumbel_encoder.layers.10.self_attn.v_proj.weight', 'gumbel_encoder.layers.5.final_layer_norm.bias', 'gumbel_encoder.layers.0.fc2.bias', 'gumbel_encoder.layers.8.self_attn.v_proj.bias', 'gumbel_encoder.layers.6.self_attn_layer_norm.bias', 'gumbel_encoder.layers.2.self_attn_layer_norm.weight', 'gumbel_encoder.layers.3.self_attn_layer_norm.weight', 'gumbel_encoder.layers.8.fc1.bias', 'gumbel_encoder.layers.2.fc2.weight', 'gumbel_encoder.layers.7.self_attn.q_proj.bias', 'gumbel_encoder.layers.0.self_attn.out_proj.weight', 'gumbel_encoder.layers.1.self_attn_layer_norm.weight', 'gumbel_encoder.layers.7.self_attn.out_proj.bias', 'gumbel_encoder.layers.1.final_layer_norm.weight', 'gumbel_encoder.layers.6.self_attn.v_proj.weight', 'gumbel_encoder.layers.11.self_attn.v_proj.weight', 'gumbel_encoder.layers.0.final_layer_norm.bias', 'gumbel_encoder.layers.8.self_attn.k_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "./src/playground.py", line 229, in <module>
    loss = forward_joint(train_data, model, train_loss_dict_, args, loss_fn, args.num_dist, tt)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/forward.py", line 23, in forward_joint
    output_en, output_l2, comm_actions, end_loss_, len_info = model(en_batch[:4],  args.sample_how)
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/bart_models.py", line 60, in forward
    spk_msg, spk_embeds, spk_cap_len_ = self.speaker(spk_h_img, a_spk_caps_in, a_spk_cap_lens) # NOTE argmax / gumbel
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/bart_models.py", line 172, in forward
    self.spk.gumbel_generate(input_images=h_img, num_beams=1, max_length=self.seq_len)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/modeling_bart.py", line 1447, in gumbel_generate
    **model_kwargs,
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/modeling_bart.py", line 1547, in gumbel_greedy_search
    outputs = self(**model_inputs, return_dict=True)
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/modeling_bart.py", line 1144, in forward
    return_dict=return_dict,
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/modeling_bart.py", line 1025, in forward
    return_dict=return_dict,
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/modeling_bart.py", line 710, in forward
    output_attentions=output_attentions,
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/modeling_bart.py", line 568, in forward
    output_attentions=output_attentions,
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/home/xuhuizh/Bart_wEye/EC_finetune/src/modeling_bart.py", line 805, in forward
    k = self.k_proj(key)
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File "/gscratch/cse/xuhuizh/anaconda3/envs/UMT/lib/python3.6/site-packages/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
RuntimeError: mat1 dim 1 must match mat2 dim 0
