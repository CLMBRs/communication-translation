# Model:
model_name: Models/bt_en-zh_baseline/best/
image_dim: 2048
hidden_dim: 512
two_ffwd: false
unit_norm: false
dropout: 0.1
fix_bhd: false
fix_spk: false
no_share_bhd: true

# Generation
decode_how: beam
beam_width: 5
temperature: 1.0
temp: 1.0
hard: false
max_seq_length: 64
TransferH: false # switch to hard gumbel after the model reaches target accuracy.

# Train and eval:
seed: 43
mode: emergent_communication
do_train: true
do_eval: true
n_gpu: 1
cpu: false
num_games: 30 # Upper bound of the num of epochs
max_global_step: 8192
lr: 3.0e-6
batch_size: 8
gradient_accumulation_steps: 4
grad_clip: 1.0
no_terminal: false
no_write: false
valid_every: 512
print_every: 32
target_acc: 85.0
stats_to_print: ['loss', 'accuracy', 'mean_length']

# Number of image distractors
num_distractors_train: 7
num_distractors_valid: 7

# Loading and saving data:
train_images: ./Data/ec_finetuning/images_train
valid_images: ./Data/ec_finetuning/images_val
save_pretrain_seperately: true
output_dir: Output/bf_accGrad4_mbart_BTen+zh_lr3.0e-6_7distractors_maxstep8192_seed43
save_output_txt: true

# Other (we may choose to delete these later):
alpha: 1.0

# Specify info about language and vocab constraint
has_vocab_constraint: false
source_lang: en_XX
source_lang_vocab_constrain_file: /projects/unmt/communication-translation/en_cc_tokenID2count_dict.cc25.json
target_lang: zh_CN
target_lang_vocab_constrain_file: /projects/unmt/communication-translation/zh-Hans_cc_tokenID2count_dict.cc25.json
