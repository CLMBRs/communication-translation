06-28-22 21:42:07 - Dataset Loaded
06-28-22 21:42:07 - Configuration:
Namespace(config='Configs/beit/en-zh_beit_ec.yml', seed_override=None, lm_lambda_override=None, drift_lambda_override=None, adapter_freeze_override=False, model_name='Output/en-zh_beit_pipeline/captions', load_entire_agent=True, image_dim=1024, reshaper_type='learned', two_ffwd=False, unit_norm=False, dropout=0.0, share_reshaper=True, recurrent_image_unroll=False, input_sequence=True, image_unroll_length=32, recurrent_hidden_aggregation=False, freeze_adapters=False, beam_width=1, temperature=1.0, hard=True, repetition_penalty=1.2, generate_from_logits=False, max_seq_length=32, TransferH=False, seed=1, mode='emergent_communication', language_model_lambda=0.125, language_model_path='Output/mbart_lm_lr6e-6', weight_drift_lambda=0.0, do_train=True, do_eval=False, n_gpu=1, num_games=30, max_global_step=2048, lr=0.0001, schedule='linear_w_warmup', num_warmup_steps=0, batch_size=12, gradient_accumulation_steps=1, grad_clip=1.0, valid_every=256, max_eval_batches=32, print_every=32, target_acc=85.0, stats_to_print=['loss', 'accuracy', 'lm loss', 'drift loss', 'communication loss', 'mean_length'], num_distractors_train=15, num_distractors_valid=15, train_images='./Data/beit_ft_ec_finetuning/images_train.pt', valid_images='./Data/beit_ft_ec_finetuning/images_val.pt', save_pretrain_seperately=True, output_dir='Output/en-zh_beit_pipeline/ec', save_output_txt=True, has_vocab_constraint=True, vocab_constraint_threshold=0.95, source_lang='en_XX', source_lang_vocab_constrain_file='./Data/cc/en_cc_tokenID2count_dict.cc25.json', target_lang='zh_CN', target_lang_vocab_constrain_file='./Data/cc/zh_cc_tokenID2count_dict.cc25.json', csv_headers=['mode', 'epoch', 'global step', 'loss', 'accuracy', 'mean_length', 'communication loss', 'lm loss'], device=device(type='cuda'))
Sharing reshaping adapter for each agent
06-28-22 21:44:54 - epoch 0 | global step 32 | mode train | loss 3.0666 | communication loss 2.7931 | accuracy 7.5521 | mean_length 24.599 | lm loss 0.2735
06-28-22 21:46:55 - epoch 0 | global step 64 | mode train | loss 2.9956 | communication loss 2.7778 | accuracy 4.9479 | mean_length 25.8438 | lm loss 0.2178
06-28-22 21:48:56 - epoch 0 | global step 96 | mode train | loss 2.9437 | communication loss 2.7694 | accuracy 8.5937 | mean_length 25.737 | lm loss 0.1743
06-28-22 21:50:57 - epoch 0 | global step 128 | mode train | loss 2.9318 | communication loss 2.777 | accuracy 3.6458 | mean_length 26.8776 | lm loss 0.1549
06-28-22 21:53:02 - epoch 0 | global step 160 | mode train | loss 2.9138 | communication loss 2.7748 | accuracy 5.2083 | mean_length 25.8802 | lm loss 0.1389
06-28-22 21:55:04 - epoch 0 | global step 192 | mode train | loss 2.9021 | communication loss 2.7704 | accuracy 8.0729 | mean_length 25.5365 | lm loss 0.1316
06-28-22 21:57:08 - epoch 0 | global step 224 | mode train | loss 2.9043 | communication loss 2.7746 | accuracy 4.6875 | mean_length 27.737 | lm loss 0.1297
06-28-22 21:59:11 - epoch 0 | global step 256 | mode train | loss 2.8967 | communication loss 2.7719 | accuracy 5.4687 | mean_length 26.7135 | lm loss 0.1247
06-28-22 21:59:54 - epoch 0 | global step 256 | mode validation | loss 2.8794 | communication loss 2.7757 | accuracy 5.9896 | mean_length 26.6979 | lm loss 0.1037
06-28-22 21:59:54 - Saving model to Output/en-zh_beit_pipeline/ec
Epoch: 0, Prediction Accuracy: 5.9896, Saved to Path: Output/en-zh_beit_pipeline/ec
06-28-22 22:02:42 - epoch 0 | global step 288 | mode train | loss 3.2336 | communication loss 2.7756 | accuracy 5.9896 | mean_length 22.1849 | lm loss 0.458
06-28-22 22:04:40 - epoch 0 | global step 320 | mode train | loss 3.1375 | communication loss 2.7771 | accuracy 3.125 | mean_length 22.9609 | lm loss 0.3604
06-28-22 22:06:38 - epoch 0 | global step 352 | mode train | loss 3.0974 | communication loss 2.7758 | accuracy 4.1667 | mean_length 23.1172 | lm loss 0.3216
06-28-22 22:08:37 - epoch 0 | global step 384 | mode train | loss 3.071 | communication loss 2.7721 | accuracy 7.5521 | mean_length 24.0729 | lm loss 0.2989
06-28-22 22:10:36 - epoch 0 | global step 416 | mode train | loss 3.0511 | communication loss 2.7714 | accuracy 5.2083 | mean_length 25.7344 | lm loss 0.2797
06-28-22 22:12:38 - epoch 0 | global step 448 | mode train | loss 3.0368 | communication loss 2.7728 | accuracy 7.0312 | mean_length 26.8255 | lm loss 0.264
06-28-22 22:14:39 - epoch 0 | global step 480 | mode train | loss 3.0306 | communication loss 2.771 | accuracy 7.8125 | mean_length 26.5495 | lm loss 0.2596
06-28-22 22:16:39 - epoch 0 | global step 512 | mode train | loss 3.0279 | communication loss 2.7729 | accuracy 5.4687 | mean_length 26.7005 | lm loss 0.255
06-28-22 22:17:22 - epoch 0 | global step 512 | mode validation | loss 3.0286 | communication loss 2.7718 | accuracy 6.7708 | mean_length 26.7161 | lm loss 0.2569
06-28-22 22:19:24 - epoch 0 | global step 544 | mode train | loss 3.0282 | communication loss 2.7741 | accuracy 5.7292 | mean_length 26.651 | lm loss 0.2542
06-28-22 22:21:24 - epoch 0 | global step 576 | mode train | loss 3.0247 | communication loss 2.7729 | accuracy 5.7292 | mean_length 26.2969 | lm loss 0.2519
06-28-22 22:23:25 - epoch 0 | global step 608 | mode train | loss 3.0185 | communication loss 2.7721 | accuracy 7.2917 | mean_length 26.3698 | lm loss 0.2464
06-28-22 22:25:25 - epoch 0 | global step 640 | mode train | loss 3.0133 | communication loss 2.7739 | accuracy 4.6875 | mean_length 26.6979 | lm loss 0.2394
06-28-22 22:27:27 - epoch 0 | global step 672 | mode train | loss 3.01 | communication loss 2.7738 | accuracy 5.9896 | mean_length 27.1615 | lm loss 0.2363
06-28-22 22:29:30 - epoch 0 | global step 704 | mode train | loss 3.0054 | communication loss 2.7744 | accuracy 7.2917 | mean_length 26.7474 | lm loss 0.231
06-28-22 22:31:30 - epoch 0 | global step 736 | mode train | loss 3.0087 | communication loss 2.7739 | accuracy 6.25 | mean_length 25.8828 | lm loss 0.2348
06-28-22 22:33:31 - epoch 0 | global step 768 | mode train | loss 2.994 | communication loss 2.773 | accuracy 6.5104 | mean_length 26.8359 | lm loss 0.221
06-28-22 22:34:18 - epoch 0 | global step 768 | mode validation | loss 2.9815 | communication loss 2.7737 | accuracy 7.5521 | mean_length 27.7917 | lm loss 0.2078
06-28-22 22:36:25 - epoch 0 | global step 800 | mode train | loss 2.9879 | communication loss 2.7752 | accuracy 5.9896 | mean_length 26.8021 | lm loss 0.2127
06-28-22 22:38:26 - epoch 0 | global step 832 | mode train | loss 2.9828 | communication loss 2.7759 | accuracy 5.4687 | mean_length 26.888 | lm loss 0.2068
06-28-22 22:40:27 - epoch 0 | global step 864 | mode train | loss 2.972 | communication loss 2.7714 | accuracy 5.9896 | mean_length 26.6849 | lm loss 0.2006
06-28-22 22:42:29 - epoch 0 | global step 896 | mode train | loss 2.9685 | communication loss 2.7733 | accuracy 5.2083 | mean_length 26.8359 | lm loss 0.1951
06-28-22 22:44:32 - epoch 0 | global step 928 | mode train | loss 2.9656 | communication loss 2.772 | accuracy 7.0312 | mean_length 26.3203 | lm loss 0.1936
06-28-22 22:46:38 - epoch 0 | global step 960 | mode train | loss 2.9678 | communication loss 2.7735 | accuracy 3.3854 | mean_length 26.8438 | lm loss 0.1943
06-28-22 22:48:46 - epoch 0 | global step 992 | mode train | loss 2.9713 | communication loss 2.773 | accuracy 5.4687 | mean_length 27.1901 | lm loss 0.1984
06-28-22 22:50:48 - epoch 0 | global step 1024 | mode train | loss 2.9781 | communication loss 2.7751 | accuracy 5.4687 | mean_length 26.3411 | lm loss 0.2031
06-28-22 22:51:30 - epoch 0 | global step 1024 | mode validation | loss 2.9724 | communication loss 2.7733 | accuracy 7.2917 | mean_length 26.5208 | lm loss 0.1992
06-28-22 22:53:32 - epoch 0 | global step 1056 | mode train | loss 2.97 | communication loss 2.7755 | accuracy 6.25 | mean_length 26.5443 | lm loss 0.1945
06-28-22 22:55:36 - epoch 0 | global step 1088 | mode train | loss 2.9639 | communication loss 2.7742 | accuracy 4.4271 | mean_length 26.349 | lm loss 0.1897
06-28-22 22:57:38 - epoch 0 | global step 1120 | mode train | loss 2.964 | communication loss 2.7727 | accuracy 7.2917 | mean_length 26.6276 | lm loss 0.1912
06-28-22 22:59:40 - epoch 0 | global step 1152 | mode train | loss 2.9878 | communication loss 2.7737 | accuracy 6.5104 | mean_length 26.2474 | lm loss 0.2141
06-28-22 23:01:42 - epoch 0 | global step 1184 | mode train | loss 2.9877 | communication loss 2.773 | accuracy 7.5521 | mean_length 26.763 | lm loss 0.2147
06-28-22 23:03:44 - epoch 0 | global step 1216 | mode train | loss 2.9684 | communication loss 2.7723 | accuracy 6.25 | mean_length 26.3411 | lm loss 0.1961
06-28-22 23:05:46 - epoch 0 | global step 1248 | mode train | loss 2.9641 | communication loss 2.773 | accuracy 8.0729 | mean_length 25.7604 | lm loss 0.1911
06-28-22 23:07:49 - epoch 0 | global step 1280 | mode train | loss 2.9576 | communication loss 2.7746 | accuracy 4.4271 | mean_length 26.4401 | lm loss 0.183
06-28-22 23:08:33 - epoch 0 | global step 1280 | mode validation | loss 2.9534 | communication loss 2.7728 | accuracy 5.9896 | mean_length 27.7266 | lm loss 0.1806
06-28-22 23:10:35 - epoch 0 | global step 1312 | mode train | loss 2.9559 | communication loss 2.7719 | accuracy 8.5938 | mean_length 26.0026 | lm loss 0.184
06-28-22 23:12:37 - epoch 0 | global step 1344 | mode train | loss 2.9598 | communication loss 2.7738 | accuracy 5.2083 | mean_length 26.1302 | lm loss 0.186
06-28-22 23:14:39 - epoch 0 | global step 1376 | mode train | loss 2.9566 | communication loss 2.773 | accuracy 5.2083 | mean_length 26.0417 | lm loss 0.1836
06-28-22 23:16:42 - epoch 0 | global step 1408 | mode train | loss 2.9557 | communication loss 2.7738 | accuracy 7.2917 | mean_length 26.9531 | lm loss 0.1818
06-28-22 23:18:44 - epoch 0 | global step 1440 | mode train | loss 2.9561 | communication loss 2.7737 | accuracy 4.9479 | mean_length 26.9323 | lm loss 0.1824
06-28-22 23:20:47 - epoch 0 | global step 1472 | mode train | loss 2.9522 | communication loss 2.772 | accuracy 5.9896 | mean_length 27.1719 | lm loss 0.1802
06-28-22 23:22:50 - epoch 0 | global step 1504 | mode train | loss 2.9519 | communication loss 2.7723 | accuracy 6.5104 | mean_length 26.513 | lm loss 0.1797
06-28-22 23:24:52 - epoch 0 | global step 1536 | mode train | loss 2.9497 | communication loss 2.7724 | accuracy 7.2917 | mean_length 26.5573 | lm loss 0.1773
06-28-22 23:25:36 - epoch 0 | global step 1536 | mode validation | loss 2.9442 | communication loss 2.7732 | accuracy 6.25 | mean_length 25.7839 | lm loss 0.1709
06-28-22 23:27:37 - epoch 0 | global step 1568 | mode train | loss 2.9481 | communication loss 2.7732 | accuracy 3.6458 | mean_length 25.401 | lm loss 0.1749
06-28-22 23:29:37 - epoch 0 | global step 1600 | mode train | loss 2.9483 | communication loss 2.7747 | accuracy 3.6458 | mean_length 24.8672 | lm loss 0.1736
06-28-22 23:31:39 - epoch 0 | global step 1632 | mode train | loss 2.9474 | communication loss 2.7714 | accuracy 8.0729 | mean_length 26.4062 | lm loss 0.176
06-28-22 23:33:42 - epoch 0 | global step 1664 | mode train | loss 2.9449 | communication loss 2.7747 | accuracy 4.1667 | mean_length 27.0547 | lm loss 0.1702
06-28-22 23:35:44 - epoch 0 | global step 1696 | mode train | loss 2.9401 | communication loss 2.7716 | accuracy 7.8125 | mean_length 26.5677 | lm loss 0.1685
06-28-22 23:37:46 - epoch 0 | global step 1728 | mode train | loss 2.955 | communication loss 2.773 | accuracy 6.25 | mean_length 26.5911 | lm loss 0.182
06-28-22 23:39:48 - epoch 0 | global step 1760 | mode train | loss 2.9484 | communication loss 2.7728 | accuracy 5.9896 | mean_length 26.8281 | lm loss 0.1756
06-28-22 23:41:51 - epoch 0 | global step 1792 | mode train | loss 2.9442 | communication loss 2.7726 | accuracy 6.5104 | mean_length 26.7656 | lm loss 0.1716
06-28-22 23:42:34 - epoch 0 | global step 1792 | mode validation | loss 2.9454 | communication loss 2.774 | accuracy 5.9896 | mean_length 27.4714 | lm loss 0.1714
06-28-22 23:44:37 - epoch 0 | global step 1824 | mode train | loss 2.9619 | communication loss 2.7721 | accuracy 5.9896 | mean_length 26.8333 | lm loss 0.1898
06-28-22 23:46:39 - epoch 0 | global step 1856 | mode train | loss 2.9669 | communication loss 2.7739 | accuracy 5.9896 | mean_length 26.3698 | lm loss 0.193
06-28-22 23:48:40 - epoch 0 | global step 1888 | mode train | loss 2.9496 | communication loss 2.7725 | accuracy 6.25 | mean_length 25.2318 | lm loss 0.1771
06-28-22 23:50:43 - epoch 0 | global step 1920 | mode train | loss 2.9518 | communication loss 2.7745 | accuracy 5.7292 | mean_length 27.2995 | lm loss 0.1772
06-28-22 23:52:45 - epoch 0 | global step 1952 | mode train | loss 2.952 | communication loss 2.7719 | accuracy 7.5521 | mean_length 26.237 | lm loss 0.1801
06-28-22 23:54:47 - epoch 0 | global step 1984 | mode train | loss 2.9506 | communication loss 2.7742 | accuracy 4.4271 | mean_length 27.0391 | lm loss 0.1764
06-28-22 23:56:49 - epoch 0 | global step 2016 | mode train | loss 2.9446 | communication loss 2.7701 | accuracy 5.2083 | mean_length 26.5625 | lm loss 0.1745
06-28-22 23:58:51 - epoch 0 | global step 2048 | mode train | loss 2.9464 | communication loss 2.7723 | accuracy 7.2917 | mean_length 26.6484 | lm loss 0.1741
06-28-22 23:59:35 - epoch 0 | global step 2048 | mode validation | loss 2.9444 | communication loss 2.7738 | accuracy 6.7708 | mean_length 27.5938 | lm loss 0.1706
