# Model:
model_name: Output/en-de_tec/captions
load_entire_agent: true
image_dim: 2048
reshaper_type: learned
two_ffwd: false
unit_norm: false
dropout: 0.0
share_reshaper: true
recurrent_image_unroll: true
image_unroll: recurrent
image_unroll_length: 32
recurrent_hidden_aggregation: true
freeze_adapters: false

# Generation
beam_width: 1
temperature: 1.0
hard: true
repetition_penalty: 1.2
generate_from_logits: false
max_seq_length: 32
TransferH: false # switch to hard gumbel after the model reaches target accuracy

# Train and eval:
seed: 1
mode: emergent_communication
ec_input_text: true
language_model_lambda: 0.0625
language_model_path: ./DataLink/../Output/mbart_lm_lr6e-6
weight_drift_lambda: 0.0
do_train: true
do_eval: false
n_gpu: 1
num_games: 30 # Upper bound of the num of epochs
max_global_step: 2048
lr: 1.0e-6
schedule: linear_w_warmup
num_warmup_steps: 0
batch_size: 12
gradient_accumulation_steps: 1
grad_clip: 0.5
valid_every: 256
max_eval_batches: 64
print_every: 32
target_acc: 85.0
stats_to_print: ['loss', 'accuracy', 'lm loss', 'drift loss', 'communication loss', 'mean_length']

# Number of image distractors
num_distractors_train: 15
num_distractors_valid: 15

# Loading and saving data:
train_captions: ./DataLink/ec_finetuning_new/en_captions_train.jsonl
valid_captions: ./DataLink/ec_finetuning_new/en_captions_val.jsonl
train_images: ./DataLink/ec_finetuning_new/images_train.pt
valid_images: ./DataLink/ec_finetuning_new/images_val.pt
save_pretrain_seperately: true
output_dir: Output/en-de_tec/ec
save_output_txt: true

# Specify info about language and vocab constraint
has_vocab_constraint: true
vocab_constraint_threshold: 0.96
source_lang: en_XX
source_lang_vocab_constrain_file: ./Data/cc/en_cc_tokenID2count_dict.cc25.json
target_lang: de_DE
target_lang_vocab_constrain_file: ./Data/cc/de_cc_tokenID2count_dict.facebook-mbart-large-cc25.json
