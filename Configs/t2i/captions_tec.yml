# Model:
model_name: facebook/mbart-large-cc25
load_entire_agent: false
image_dim: 2048
reshaper_type: learned
two_ffwd: false
unit_norm: false
dropout: 0.0
share_reshaper: true
image_unroll: recurrent
image_unroll_length: 32
recurrent_hidden_aggregation: true
freeze_adapters: false

# Generation
beam_width: 1
temperature: 1.0
hard: true
max_seq_length: 32
max_text_seq_length: 128
repetition_penalty: 1.0
generate_from_logits: false
TransferH: false # switch to hard gumbel after the model reaches target accuracy

# Train and eval:
seed: 1
mode: image_grounding
use_caption_crossattention: true
image_selection_lambda: 8.0
language_model_lambda: 0.0
weight_drift_lambda: 0.0
do_train: true
do_eval: true
n_gpu: 1
num_games: 30 # Upper bound of the num of epochs
max_global_step: 2048
lr: 4.0e-5
schedule: linear_w_warmup
num_warmup_steps: 0
gradient_accumulation_steps: 1
batch_size: 16
grad_clip: 0.5
valid_every: 512
print_every: 32
target_acc: 85.0
stats_to_print: ['loss', 'accuracy', 'caption generation loss', 'image selection loss']

# Number of image distractors
num_distractors_train: 7
num_distractors_valid: 7

# Loading and saving data:
train_captions: ./DataLink/captioning_new/en_captions_train.jsonl
valid_captions: ./DataLink/captioning_new/en_captions_val.jsonl
train_images: ./DataLink/captioning_new/images_train.pt
valid_images: ./DataLink/captioning_new/images_val.pt
save_pretrain_seperately: true
output_dir: Output/en-de_tec/captions
save_output_txt: true

# Specify info about language and vocab constraint
has_vocab_constraint: false
source_lang: en_XX
source_lang_vocab_constrain_file: /projects/unmt/communication-translation/en_cc_tokenID2count_dict.cc25.json
