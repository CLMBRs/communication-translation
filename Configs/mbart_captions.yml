# Model:
model_name: mbart
image_dim: 2048
hidden_dim: 512
bidirectional: false # RNN-specific
num_layers: 1 # RNN-specific
vocab_size: 4035 # RNN-specific
two_ffwd: false
unit_norm: false
dropout: 0.1
fix_bhd: false
fix_spk: false
no_share_bhd: false

# Generation
decode_how: beam
beam_width: 12
temperature: 1.0
temp: 1.0
hard: true
max_seq_length: 128 # sequence length limit
TransferH: false #switch to hard gumbel after the model reaches target accuracy.

# Train and eval:
seed: 42
do_train: true # control whether we train
do_eval: true # whether to do eval after training
n_gpu: 1
cpu: false
num_games: 30 # Upper bound of the num of epochs
max_global_step: 30000
lr: 1.0e-5
batch_size: 64
grad_clip: 1.0
no_terminal: false
no_write: false
valid_every: 500
print_every: 50
target_acc: 85.0

# Number of image distractors
num_distractors_train: 7
num_distractors_valid: 7

# Loading and saving data:
train_captions: ./Data/Coco/captioning/en_train_captions.jsonl
valid_captions: ./Data/Coco/captioning/en_valid_captions.jsonl
train_images: ./Data/Coco/captioning/train_images
valid_images: ./Data/Coco/captioning/valid_images
save_pretrain_seperately: true
# good practice: set the output_dir name the same as the parameter file name
output_dir: mbart_captions
save_output_txt: true

# Other (we may choose to delete these later):
alpha: 1.0

# Specify info about language and vocab constraint
has_vocab_constraint: true
source_lang: en_XX
source_lang_vocab_constrain_file: /projects/unmt/communication-translation/en_cc_tokenID2count_dict.cc25.json
target_lang: zh_CN
target_lang_vocab_constrain_file: /projects/unmt/communication-translation/zh-Hans_cc_tokenID2count_dict.cc25.json
