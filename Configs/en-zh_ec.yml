# Model:
model_name: Output/en-zh_pipeline/last
load_entire_agent: true
image_dim: 2048
hidden_dim: 512
bidirectional: false # RNN-specific
num_layers: 1 # RNN-specific
vocab_size: 4035 # RNN-specific
two_ffwd: false
unit_norm: false
dropout: 0.1
fix_bhd: false
fix_spk: false
no_share_bhd: true
recurrent_image_unroll: true
recurrent_hidden_aggregation: true

# Generation
decode_how: beam
beam_width: 1
temperature: 1.0
hard: true
repetition_penalty: 2.0
max_seq_length: 64
TransferH: false # switch to hard gumbel after the model reaches target accuracy

# Train and eval:
seed: 1
mode: emergent_communication
do_train: true
do_eval: true
n_gpu: 1
cpu: false
num_games: 30 # Upper bound of the num of epochs
max_global_step: 512
lr: 8.0e-6
schedule: constant_w_warmup
num_warmup_steps: 0
batch_size: 12
gradient_accumulation_steps: 1
grad_clip: 1.0
no_terminal: false
no_write: false
valid_every: 512
print_every: 32
target_acc: 85.0
stats_to_print: ['loss', 'accuracy', 'caption generation loss', 'image selection loss', 'mean_length']

# Number of image distractors
num_distractors_train: 15
num_distractors_valid: 15

# Loading and saving data:
# train_captions: ./Data/Coco/captioning/en_train_captions.jsonl
# valid_captions: ./Data/Coco/captioning/en_valid_captions.jsonl
train_images: ./Data/ec_finetuning/images_train
valid_images: ./Data/ec_finetuning/images_val
save_pretrain_seperately: true
output_dir: Output/en-zh_pipeline/last
save_output_txt: true

# Other (we may choose to delete these later):
alpha: 1.0

# Specify info about language and vocab constraint
has_vocab_constraint: true
source_lang: en_XX
source_lang_vocab_constrain_file: ./Data/cc/en_cc_tokenID2count_dict.cc25.json
target_lang: zh_CN
target_lang_vocab_constrain_file: ./Data/cc/zh_cc_tokenID2count_dict.cc25.json
