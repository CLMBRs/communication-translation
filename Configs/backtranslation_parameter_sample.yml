# Path to load model for generation
model_path: Data/mbart-large-cc25
#args_path: Output/mbart_wcons/training_args.bin

# training config
lr: 1.0e-05
batch_size: 2
seed: 42
n_gpu: 1
print_every: 5
do_validation: true
validate_every: 5
# validation_set_size is a lower bound of how many example used
# ceil(validation_set_size / batch_size) batches (of size batch_size) are run.
validation_set_size: 50

num_steps: 13
# script to load validation datasets
val_dataset_script: BackTranslation/wmt19/wmt19.py
patience: 5
# dir to save model from lang1 to lang2
output_dir: Output/bt_en+zh/
# dir to save model from lang2 to lang1

# data config
data_dir: ./Data/cc
lang1_data_file: en_head2000.txt
lang1_id: en
lang1_max_len: 10
lang1_vocab_constrain_file: ./Data/cc/en_cc_tokenID2count_dict.cc25.json

lang2_data_file: zh_head2000.txt
lang2_id: zh
lang2_max_len: 10
lang2_vocab_constrain_file: ./Data/cc/zh_cc_tokenID2count_dict.cc25.json

lang_pair: zh-en
# backtranslation language
models_shared: true
