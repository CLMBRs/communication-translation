# Hyper-parameters for the referential games 
embedding_dim: 1024
hidden_dim: 512
image_dim: 2048
ECemb: 5000
seed: 42
# Setting the n_gpu we use
n_gpu: 1
# Setting the model to use: 
model: mbart
results_save_path: Output
TransferH: false
alpha: 1.0
batch_size: 16
beam_width: 12
cpu: True
dataset: coco
coco_path: Data/coco_new
decode_how: beam
dropout: 0.1
eval_mode: false
fix_bhd: false
fix_spk: false
gpuid: 0
grad_clip: 1.0
hard: false
len_loss: false
loss_type: xent
lr: 1.0e-05
no_share_bhd: false
no_terminal: false
no_write: false
norm_pow: 0.0
bidirectional: false

# Total number of pictures for listeners, the first one is for training. 
num_distractors_train: 255
num_distractors_valid: 127
num_games: 30000
num_layers: 1
pretrain_spk: false
print_every: 50
re_load: false
sample_how: gumbel
save_every: 4000
seq_len: 15
stop_after: 30
temperature: 1.0
translate_every: 2000
two_ffwd: false
unit_norm: false
valid_batch_size: 128
valid_every: 250
vocab_size: 4035
which_loss: joint

# specify info about language and vocab constraint
source_lang: en_XX
source_lang_vocab_constrain_file: ./Data/cc/en_europarl_count_dict.json
target_lang: ja_XX
target_lang_vocab_constrain_file: ./Data/cc/ja_smaller_count_dict.json
