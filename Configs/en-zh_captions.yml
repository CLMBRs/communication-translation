# Model:
model_name: Output/bt_en-zh_baseline/best
load_entire_agent: false
image_dim: 2048
hidden_dim: 512
bidirectional: false # RNN-specific
num_layers: 1 # RNN-specific
vocab_size: 4035 # RNN-specific
two_ffwd: false
unit_norm: false
dropout: 0.1
fix_bhd: false
fix_spk: false
no_share_bhd: true
recurrent_image_unroll: true
recurrent_hidden_aggregation: true

# Generation
decode_how: beam
beam_width: 5
temperature: 1.0
temp: 1.0
hard: false
max_seq_length: 64
TransferH: false # switch to hard gumbel after the model reaches target accuracy

# Train and eval:
seed: 1
mode: image_grounding
do_train: true
do_eval: true
n_gpu: 1
cpu: false
num_games: 30 # Upper bound of the num of epochs
max_global_step: 2048
lr: 1.0e-5
schedule: constant_w_warmup
num_warmup_steps: 0
gradient_accumulation_steps: 1
batch_size: 16
grad_clip: 1.0
no_terminal: false
no_write: false
valid_every: 512
print_every: 32
target_acc: 85.0
stats_to_print: ['loss', 'accuracy', 'caption generation loss', 'image selection loss']

# Number of image distractors
num_distractors_train: 15
num_distractors_valid: 15

# Loading and saving data:
train_captions: ./Data/captioning/en_captions_train.jsonl
valid_captions: ./Data/captioning/en_captions_val.jsonl
train_images: ./Data/captioning/images_train
valid_images: ./Data/captioning/images_val
save_pretrain_seperately: true
output_dir: Output/bt_en-zh_captions
save_output_txt: true

# Other (we may choose to delete these later):
alpha: 1.0

# Specify info about language and vocab constraint
has_vocab_constraint: false
source_lang: en_XX
source_lang_vocab_constrain_file: /projects/unmt/communication-translation/en_cc_tokenID2count_dict.cc25.json
target_lang: zh_CN
target_lang_vocab_constrain_file: /projects/unmt/communication-translation/zh-Hans_cc_tokenID2count_dict.cc25.json
